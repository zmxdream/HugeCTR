<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-GPU Offline Inference &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Parameter Server Demo" href="hps_demo.html" />
    <link rel="prev" title="HugeCTR Python Interface" href="hugectr_criteo.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HugeCTR Library</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_feature_details_intro.html">Features in Detail</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../hugectr_example_notebooks.html">Example Notebooks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="hugectr_wdl_prediction.html">HugeCTR Wide and Deep Model with Criteo</a></li>
<li class="toctree-l2"><a class="reference internal" href="hugectr2onnx_demo.html">HugeCTR to ONNX Converter</a></li>
<li class="toctree-l2"><a class="reference internal" href="continuous_training.html">HugeCTR Continuous Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecommerce-example.html">Merlin ETL, training and inference demo on the e-Commerce behavior data</a></li>
<li class="toctree-l2"><a class="reference internal" href="movie-lens-example.html">HugeCTR demo on Movie lens data</a></li>
<li class="toctree-l2"><a class="reference internal" href="hugectr_criteo.html">HugeCTR Python Interface</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Multi-GPU Offline Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Table-of-Contents">Table of Contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hps_demo.html">Hierarchical Parameter Server Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/00-Intro.html">Training Recommender Systems on Multi-modal Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/01-Download-Convert.html">MovieLens-25M: Download and Convert</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/03-Feature-Extraction-Poster.html">Movie Poster Feature Extraction with ResNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/04-Feature-Extraction-Text.html">Movie Synopsis Feature Extraction with Bart text summarization</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/04-Feature-Extraction-Text.html#Download-pretrained-BART-model">Download pretrained BART model</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/05-Create-Feature-Store.html">Creating Multi-Modal Movie Feature Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/06-ETL-with-NVTabular.html">ETL with NVTabular</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-modal-data/07-Training-with-HugeCTR.html">Training HugeCTR Model with Pretrained Embeddings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../additional_resources.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../hugectr_example_notebooks.html">HugeCTR Example Notebooks</a> &raquo;</li>
      <li>Multi-GPU Offline Inference</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><img alt="c3aa769413dd405e801b4c2ccc0e2463" src="http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png" /></p>
<div class="section" id="Multi-GPU-Offline-Inference">
<h1>Multi-GPU Offline Inference<a class="headerlink" href="#Multi-GPU-Offline-Inference" title="Permalink to this headline"></a></h1>
<div class="section" id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Permalink to this headline"></a></h2>
<p>In HugeCTR version 3.4.1, we provide Python APIs to do multi-GPU offline inference, which leverage <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_core_features.html#hierarchical-parameter-server">HugeCTR Hierarchical Parameter Server</a> and enable concurrent execution on multiple devices. The Norm or Parquet dataset is currently supported by multi-GPU offline inference.</p>
<p>This notebook explains how to do multi-GPU offline inference with HugeCTR Python APIs. For more details, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#inference-api">HugeCTR Python Interface</a>.</p>
</div>
<div class="section" id="Table-of-Contents">
<h2>Table of Contents<a class="headerlink" href="#Table-of-Contents" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#1">Installation</a></p>
<ul>
<li><p><a class="reference external" href="#11">Get HugeCTR from NGC</a></p></li>
<li><p><a class="reference external" href="#12">Build HugeCTR from Source Code</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#2">Demo</a></p>
<ul>
<li><p><a class="reference external" href="#21">Data Generation</a></p></li>
<li><p><a class="reference external" href="#22">Train from Scratch</a></p></li>
<li><p><a class="reference external" href="#23">Multi-GPU Offline Inference</a></p></li>
</ul>
</li>
</ul>
<p>## 1. Installation</p>
<p>### 1.1 Get HugeCTR from NGC The HugeCTR Python module is preinstalled in the <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-training">Merlin Training Container</a>: <code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia/merlin/merlin-training:22.04</span></code>.</p>
<p>You can check the existence of required libraries by running the following Python code after launching this container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ python3 -c <span class="s2">&quot;import hugectr&quot;</span>
</pre></div>
</div>
<p><strong>Note</strong>: This Python module contains both training APIs and offline inference APIs. For online inference with Triton, please refer to <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend">HugeCTR Backend</a>.</p>
<p>### 1.2 Build HugeCTR from Source Code</p>
<p>If you want to build HugeCTR from the source code instead of using the NGC container, please refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html#how-to-start-your-development">How to Start Your Development</a>.</p>
<p>## 2. Demo</p>
<p>### 2.1 Data Generation HugeCTR provides a tool to generate synthetic datasets. The <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api">Data Generator</a> is capable of generating datasets of different formats and distributions. We will generate multi-hot Parquet datasets with power law distribution for this notebook:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import hugectr
from hugectr.tools import DataGeneratorParams, DataGenerator

data_generator_params = DataGeneratorParams(
  format = hugectr.DataReaderType_t.Parquet,
  label_dim = 2,
  dense_dim = 2,
  num_slot = 3,
  i64_input_key = True,
  nnz_array = [2, 1, 3],
  source = &quot;./multi_hot_parquet/file_list.txt&quot;,
  eval_source = &quot;./multi_hot_parquet/file_list_test.txt&quot;,
  slot_size_array = [10000, 10000, 10000],
  check_type = hugectr.Check_t.Non,
  dist_type = hugectr.Distribution_t.PowerLaw,
  power_law_type = hugectr.PowerLaw_t.Short,
  num_files = 16,
  eval_num_files = 4)
data_generator = DataGenerator(data_generator_params)
data_generator.generate()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[HCTR][15:01:03][INFO][RK0][main]: Generate Parquet dataset
[HCTR][15:01:03][INFO][RK0][main]: train data folder: ./multi_hot_parquet, eval data folder: ./multi_hot_parquet, slot_size_array: 10000, 10000, 10000, nnz array: 2, 1, 3, #files for train: 16, #files for eval: 4, #samples per file: 40960, Use power law distribution: 1, alpha of power law: 1.3
[HCTR][15:01:03][INFO][RK0][main]: ./multi_hot_parquet exist
[HCTR][15:01:03][INFO][RK0][main]: ./multi_hot_parquet/train/gen_0.parquet
[HCTR][15:01:05][INFO][RK0][main]: ./multi_hot_parquet/train/gen_1.parquet
[HCTR][15:01:05][INFO][RK0][main]: ./multi_hot_parquet/train/gen_2.parquet
[HCTR][15:01:05][INFO][RK0][main]: ./multi_hot_parquet/train/gen_3.parquet
[HCTR][15:01:05][INFO][RK0][main]: ./multi_hot_parquet/train/gen_4.parquet
[HCTR][15:01:05][INFO][RK0][main]: ./multi_hot_parquet/train/gen_5.parquet
[HCTR][15:01:05][INFO][RK0][main]: ./multi_hot_parquet/train/gen_6.parquet
[HCTR][15:01:06][INFO][RK0][main]: ./multi_hot_parquet/train/gen_7.parquet
[HCTR][15:01:06][INFO][RK0][main]: ./multi_hot_parquet/train/gen_8.parquet
[HCTR][15:01:06][INFO][RK0][main]: ./multi_hot_parquet/train/gen_9.parquet
[HCTR][15:01:06][INFO][RK0][main]: ./multi_hot_parquet/train/gen_10.parquet
[HCTR][15:01:06][INFO][RK0][main]: ./multi_hot_parquet/train/gen_11.parquet
[HCTR][15:01:06][INFO][RK0][main]: ./multi_hot_parquet/train/gen_12.parquet
[HCTR][15:01:07][INFO][RK0][main]: ./multi_hot_parquet/train/gen_13.parquet
[HCTR][15:01:07][INFO][RK0][main]: ./multi_hot_parquet/train/gen_14.parquet
[HCTR][15:01:07][INFO][RK0][main]: ./multi_hot_parquet/train/gen_15.parquet
[HCTR][15:01:07][INFO][RK0][main]: ./multi_hot_parquet/file_list.txt done!
[HCTR][15:01:07][INFO][RK0][main]: ./multi_hot_parquet/val/gen_0.parquet
[HCTR][15:01:07][INFO][RK0][main]: ./multi_hot_parquet/val/gen_1.parquet
[HCTR][15:01:08][INFO][RK0][main]: ./multi_hot_parquet/val/gen_2.parquet
[HCTR][15:01:08][INFO][RK0][main]: ./multi_hot_parquet/val/gen_3.parquet
[HCTR][15:01:08][INFO][RK0][main]: ./multi_hot_parquet/file_list_test.txt done!
</pre></div></div>
</div>
<p>### 2.2 Train from Scratch We can train fom scratch by doing the following steps with Python APIs:</p>
<ol class="arabic simple">
<li><p>Create the solver, reader and optimizer, then initialize the model.</p></li>
<li><p>Construct the model graph by adding input, sparse embedding and dense layers in order.</p></li>
<li><p>Compile the model and have an overview of the model graph.</p></li>
<li><p>Dump the model graph to the JSON file.</p></li>
<li><p>Fit the model, save the model weights and optimizer states implicitly.</p></li>
<li><p>Dump one batch of evaluation results to files.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%writefile multi_hot_train.py
import hugectr
from mpi4py import MPI
solver = hugectr.CreateSolver(model_name = &quot;multi_hot&quot;,
                              max_eval_batches = 1,
                              batchsize_eval = 16384,
                              batchsize = 16384,
                              lr = 0.001,
                              vvgpu = [[0]],
                              i64_input_key = True,
                              repeat_dataset = True,
                              use_cuda_graph = True)
reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,
                                  source = [&quot;./multi_hot_parquet/file_list.txt&quot;],
                                  eval_source = &quot;./multi_hot_parquet/file_list_test.txt&quot;,
                                  check_type = hugectr.Check_t.Non,
                                  slot_size_array = [10000, 10000, 10000])
optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)
model = hugectr.Model(solver, reader, optimizer)
model.add(hugectr.Input(label_dim = 2, label_name = &quot;label&quot;,
                        dense_dim = 2, dense_name = &quot;dense&quot;,
                        data_reader_sparse_param_array =
                        [hugectr.DataReaderSparseParam(&quot;data1&quot;, [2, 1], False, 2),
                        hugectr.DataReaderSparseParam(&quot;data2&quot;, 3, False, 1),]))
model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash,
                            workspace_size_per_gpu_in_mb = 4,
                            embedding_vec_size = 16,
                            combiner = &quot;sum&quot;,
                            sparse_embedding_name = &quot;sparse_embedding1&quot;,
                            bottom_name = &quot;data1&quot;,
                            optimizer = optimizer))
model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash,
                            workspace_size_per_gpu_in_mb = 2,
                            embedding_vec_size = 16,
                            combiner = &quot;sum&quot;,
                            sparse_embedding_name = &quot;sparse_embedding2&quot;,
                            bottom_name = &quot;data2&quot;,
                            optimizer = optimizer))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,
                            bottom_names = [&quot;sparse_embedding1&quot;],
                            top_names = [&quot;reshape1&quot;],
                            leading_dim=32))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,
                            bottom_names = [&quot;sparse_embedding2&quot;],
                            top_names = [&quot;reshape2&quot;],
                            leading_dim=16))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,
                            bottom_names = [&quot;reshape1&quot;, &quot;reshape2&quot;, &quot;dense&quot;], top_names = [&quot;concat1&quot;]))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,
                            bottom_names = [&quot;concat1&quot;],
                            top_names = [&quot;fc1&quot;],
                            num_output=1024))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,
                            bottom_names = [&quot;fc1&quot;],
                            top_names = [&quot;relu1&quot;]))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,
                            bottom_names = [&quot;relu1&quot;],
                            top_names = [&quot;fc2&quot;],
                            num_output=2))
model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCrossEntropyLoss,
                            bottom_names = [&quot;fc2&quot;, &quot;label&quot;],
                            top_names = [&quot;loss&quot;],
                            target_weight_vec = [0.5, 0.5]))
model.compile()
model.summary()
model.graph_to_json(&quot;multi_hot.json&quot;)
model.fit(max_iter = 1100, display = 200, eval_interval = 1000, snapshot = 1000, snapshot_prefix = &quot;multi_hot&quot;)
model.export_predictions(&quot;multi_hot_pred_&quot; + str(1000), &quot;multi_hot_label_&quot; + str(1000))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Overwriting multi_hot_train.py
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!python3 multi_hot_train.py
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
HugeCTR Version: 3.4
====================================================Model Init=====================================================
[HCTR][15:04:04][INFO][RK0][main]: Initialize model: multi_hot
[HCTR][15:04:04][INFO][RK0][main]: Global seed is 2258929170
[HCTR][15:04:04][INFO][RK0][main]: Device to NUMA mapping:
  GPU 0 -&gt;  node 0
[HCTR][15:04:05][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.
[HCTR][15:04:05][INFO][RK0][main]: Start all2all warmup
[HCTR][15:04:05][INFO][RK0][main]: End all2all warmup
[HCTR][15:04:05][INFO][RK0][main]: Using All-reduce algorithm: NCCL
[HCTR][15:04:05][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB
[HCTR][15:04:05][INFO][RK0][main]: num of DataReader workers: 1
[HCTR][15:04:05][INFO][RK0][main]: Vocabulary size: 30000
[HCTR][15:04:05][INFO][RK0][main]: max_vocabulary_size_per_gpu_=65536
[HCTR][15:04:05][INFO][RK0][main]: max_vocabulary_size_per_gpu_=32768
[HCTR][15:04:05][INFO][RK0][main]: Graph analysis to resolve tensor dependency
===================================================Model Compile===================================================
[HCTR][15:04:14][INFO][RK0][main]: gpu0 start to init embedding
[HCTR][15:04:14][INFO][RK0][main]: gpu0 init embedding done
[HCTR][15:04:14][INFO][RK0][main]: gpu0 start to init embedding
[HCTR][15:04:14][INFO][RK0][main]: gpu0 init embedding done
[HCTR][15:04:14][INFO][RK0][main]: Starting AUC NCCL warm-up
[HCTR][15:04:14][INFO][RK0][main]: Warm-up done
[HCTR][15:04:14][INFO][RK0][main]: ===================================================Model Summary===================================================
label                                   Dense                         Sparse
label                                   dense                          data1,data2
(None, 2)                               (None, 2)
——————————————————————————————————————————————————————————————————————————————————————————————————————————————————
Layer Type                              Input Name                    Output Name                   Output Shape
——————————————————————————————————————————————————————————————————————————————————————————————————————————————————
DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 2, 16)
------------------------------------------------------------------------------------------------------------------
DistributedSlotSparseEmbeddingHash      data2                         sparse_embedding2             (None, 1, 16)
------------------------------------------------------------------------------------------------------------------
Reshape                                 sparse_embedding1             reshape1                      (None, 32)
------------------------------------------------------------------------------------------------------------------
Reshape                                 sparse_embedding2             reshape2                      (None, 16)
------------------------------------------------------------------------------------------------------------------
Concat                                  reshape1                      concat1                       (None, 50)
                                        reshape2
                                        dense
------------------------------------------------------------------------------------------------------------------
InnerProduct                            concat1                       fc1                           (None, 1024)
------------------------------------------------------------------------------------------------------------------
ReLU                                    fc1                           relu1                         (None, 1024)
------------------------------------------------------------------------------------------------------------------
InnerProduct                            relu1                         fc2                           (None, 2)
------------------------------------------------------------------------------------------------------------------
MultiCrossEntropyLoss                   fc2                           loss
                                        label
------------------------------------------------------------------------------------------------------------------
[HCTR][15:04:14][INFO][RK0][main]: Save the model graph to multi_hot.json successfully
=====================================================Model Fit=====================================================
[HCTR][15:04:14][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1100
[HCTR][15:04:14][INFO][RK0][main]: Training batchsize: 16384, evaluation batchsize: 16384
[HCTR][15:04:14][INFO][RK0][main]: Evaluation interval: 1000, snapshot interval: 1000
[HCTR][15:04:14][INFO][RK0][main]: Dense network trainable: True
[HCTR][15:04:14][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True
[HCTR][15:04:14][INFO][RK0][main]: Sparse embedding sparse_embedding2 trainable: True
[HCTR][15:04:14][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True
[HCTR][15:04:14][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000
[HCTR][15:04:14][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000
[HCTR][15:04:14][INFO][RK0][main]: Training source file: ./multi_hot_parquet/file_list.txt
[HCTR][15:04:14][INFO][RK0][main]: Evaluation source file: ./multi_hot_parquet/file_list_test.txt
[HCTR][15:04:17][INFO][RK0][main]: Iter: 200 Time(200 iters): 2.73086s Loss: 0.342286 lr:0.001
[HCTR][15:04:20][INFO][RK0][main]: Iter: 400 Time(200 iters): 2.57674s Loss: 0.339907 lr:0.001
[HCTR][15:04:22][INFO][RK0][main]: Iter: 600 Time(200 iters): 2.59306s Loss: 0.338068 lr:0.001
[HCTR][15:04:25][INFO][RK0][main]: Iter: 800 Time(200 iters): 2.56907s Loss: 0.334571 lr:0.001
[HCTR][15:04:27][INFO][RK0][main]: Iter: 1000 Time(200 iters): 2.57584s Loss: 0.331733 lr:0.001
[HCTR][15:04:27][INFO][RK0][main]: Evaluation, AUC: 0.500278
[HCTR][15:04:27][INFO][RK0][main]: Eval Time for 1 iters: 0.001344s
[HCTR][15:04:27][INFO][RK0][main]: Rank0: Write hash table to file
[HCTR][15:04:27][INFO][RK0][main]: Rank0: Write hash table to file
[HCTR][15:04:27][INFO][RK0][main]: Dumping sparse weights to files, successful
[HCTR][15:04:27][INFO][RK0][main]: Rank0: Write optimzer state to file
[HCTR][15:04:27][INFO][RK0][main]: Done
[HCTR][15:04:27][INFO][RK0][main]: Rank0: Write optimzer state to file
[HCTR][15:04:27][INFO][RK0][main]: Done
[HCTR][15:04:28][INFO][RK0][main]: Rank0: Write optimzer state to file
[HCTR][15:04:28][INFO][RK0][main]: Done
[HCTR][15:04:28][INFO][RK0][main]: Rank0: Write optimzer state to file
[HCTR][15:04:28][INFO][RK0][main]: Done
[HCTR][15:04:28][INFO][RK0][main]: Dumping sparse optimzer states to files, successful
[HCTR][15:04:28][INFO][RK0][main]: Dumping dense weights to file, successful
[HCTR][15:04:28][INFO][RK0][main]: Dumping dense optimizer states to file, successful
[HCTR][15:04:29][INFO][RK0][main]: Finish 1100 iterations with batchsize: 16384 in 14.54s.
</pre></div></div>
</div>
<p>### 2.3 Multi-GPU Offline Inference</p>
<p>We will demonstrate multi-GPU offline inference by doing the following steps with Python APIs: 1. Configure the inference hyperparameters. 2. Initialize the inference model, which is a collection of inference sessions deployed on multiple devices. 3. Make inference from the evaluation dataset. 4. Check the correctness by comparing with dumped evaluation results.</p>
<p><strong>Note</strong>: The <code class="docutils literal notranslate"><span class="pre">max_batchsize</span></code> configured within <code class="docutils literal notranslate"><span class="pre">InferenceParams</span></code> is the global batch size, and it should be divisible by the number of deployed devices. The numpy array returned by <code class="docutils literal notranslate"><span class="pre">InferenceModel.predict</span></code> is of the shape <code class="docutils literal notranslate"><span class="pre">(max_batchsize</span> <span class="pre">*</span> <span class="pre">num_batches,</span> <span class="pre">label_dim)</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import hugectr
from hugectr.inference import InferenceModel, InferenceParams
import numpy as np
from mpi4py import MPI

model_config = &quot;multi_hot.json&quot;
inference_params = InferenceParams(
    model_name = &quot;multi_hot&quot;,
    max_batchsize = 1024,
    hit_rate_threshold = 1.0,
    dense_model_file = &quot;multi_hot_dense_1000.model&quot;,
    sparse_model_files = [&quot;multi_hot0_sparse_1000.model&quot;, &quot;multi_hot1_sparse_1000.model&quot;],
    deployed_devices = [0, 1, 2, 3],
    use_gpu_embedding_cache = True,
    cache_size_percentage = 0.5,
    i64_input_key = True
)
inference_model = InferenceModel(model_config, inference_params)
pred = inference_model.predict(
    16,
    &quot;./multi_hot_parquet/file_list_test.txt&quot;,
    hugectr.DataReaderType_t.Parquet,
    hugectr.Check_t.Non,
    [10000, 10000, 10000]
)
grount_truth = np.loadtxt(&quot;multi_hot_pred_1000&quot;)
print(&quot;pred: &quot;, pred)
print(&quot;grount_truth: &quot;, grount_truth)
diff = pred.flatten()-grount_truth
mse = np.mean(diff*diff)
print(&quot;mse: &quot;, mse)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[HCTR][15:04:58][INFO][RK0][main]: Global seed is 3101700364
[HCTR][15:04:58][INFO][RK0][main]: Device to NUMA mapping:
  GPU 0 -&gt;  node 0
  GPU 1 -&gt;  node 0
  GPU 2 -&gt;  node 0
  GPU 3 -&gt;  node 0
[HCTR][15:05:01][INFO][RK0][main]: Start all2all warmup
[HCTR][15:05:02][INFO][RK0][main]: End all2all warmup
[HCTR][15:05:02][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0
[HCTR][15:05:02][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0
[HCTR][15:05:02][INFO][RK0][main]: Creating ParallelHashMap CPU database backend...
[HCTR][15:05:02][INFO][RK0][main]: Created parallel (16 partitions) blank database backend in local memory!
[HCTR][15:05:02][INFO][RK0][main]: Volatile DB: initial cache rate = 1
[HCTR][15:05:02][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0
[HCTR][15:05:02][INFO][RK0][main]: Table: hctr_et.multi_hot.sparse_embedding1; cached 16597 / 16597 embeddings in volatile database (ParallelHashMap); load: 16597 / 18446744073709551615 (0.00%).
[HCTR][15:05:02][INFO][RK0][main]: Table: hctr_et.multi_hot.sparse_embedding2; cached 9253 / 9253 embeddings in volatile database (ParallelHashMap); load: 9253 / 18446744073709551615 (0.00%).
[HCTR][15:05:02][DEBUG][RK0][main]: Real-time subscribers created!
[HCTR][15:05:02][INFO][RK0][main]: Create embedding cache in device 0.
[HCTR][15:05:02][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000
[HCTR][15:05:02][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000
[HCTR][15:05:02][INFO][RK0][main]: Create embedding cache in device 1.
[HCTR][15:05:02][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000
[HCTR][15:05:02][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000
[HCTR][15:05:02][INFO][RK0][main]: Create embedding cache in device 2.
[HCTR][15:05:02][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000
[HCTR][15:05:02][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000
[HCTR][15:05:02][INFO][RK0][main]: Create embedding cache in device 3.
[HCTR][15:05:02][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.500000
[HCTR][15:05:02][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000
[HCTR][15:05:02][INFO][RK0][main]: Global seed is 1801008028
[HCTR][15:05:02][INFO][RK0][main]: Device to NUMA mapping:
  GPU 0 -&gt;  node 0
[HCTR][15:05:02][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.
[HCTR][15:05:02][INFO][RK0][main]: Start all2all warmup
[HCTR][15:05:02][INFO][RK0][main]: End all2all warmup
[HCTR][15:05:02][INFO][RK0][main]: Create inference session on device: 0
[HCTR][15:05:02][INFO][RK0][main]: Model name: multi_hot
[HCTR][15:05:02][INFO][RK0][main]: Use mixed precision: False
[HCTR][15:05:02][INFO][RK0][main]: Use cuda graph: True
[HCTR][15:05:02][INFO][RK0][main]: Max batchsize: 256
[HCTR][15:05:02][INFO][RK0][main]: Use I64 input key: True
[HCTR][15:05:02][INFO][RK0][main]: start create embedding for inference
[HCTR][15:05:02][INFO][RK0][main]: sparse_input name data1
[HCTR][15:05:02][INFO][RK0][main]: sparse_input name data2
[HCTR][15:05:02][INFO][RK0][main]: create embedding for inference success
[HCTR][15:05:02][INFO][RK0][main]: Inference stage skip MultiCrossEntropyLoss layer, replaced by Sigmoid layer
[HCTR][15:05:02][INFO][RK0][main]: Global seed is 1395008125
[HCTR][15:05:02][INFO][RK0][main]: Device to NUMA mapping:
  GPU 1 -&gt;  node 0
[HCTR][15:05:02][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.
[HCTR][15:05:02][INFO][RK0][main]: Start all2all warmup
[HCTR][15:05:02][INFO][RK0][main]: End all2all warmup
[HCTR][15:05:02][INFO][RK0][main]: Create inference session on device: 1
[HCTR][15:05:02][INFO][RK0][main]: Model name: multi_hot
[HCTR][15:05:02][INFO][RK0][main]: Use mixed precision: False
[HCTR][15:05:02][INFO][RK0][main]: Use cuda graph: True
[HCTR][15:05:02][INFO][RK0][main]: Max batchsize: 256
[HCTR][15:05:02][INFO][RK0][main]: Use I64 input key: True
[HCTR][15:05:02][INFO][RK0][main]: start create embedding for inference
[HCTR][15:05:02][INFO][RK0][main]: sparse_input name data1
[HCTR][15:05:02][INFO][RK0][main]: sparse_input name data2
[HCTR][15:05:02][INFO][RK0][main]: create embedding for inference success
[HCTR][15:05:02][INFO][RK0][main]: Inference stage skip MultiCrossEntropyLoss layer, replaced by Sigmoid layer
[HCTR][15:05:02][INFO][RK0][main]: Global seed is 3124827580
[HCTR][15:05:02][INFO][RK0][main]: Device to NUMA mapping:
  GPU 2 -&gt;  node 0
[HCTR][15:05:03][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.
[HCTR][15:05:03][INFO][RK0][main]: Start all2all warmup
[HCTR][15:05:03][INFO][RK0][main]: End all2all warmup
[HCTR][15:05:03][INFO][RK0][main]: Create inference session on device: 2
[HCTR][15:05:03][INFO][RK0][main]: Model name: multi_hot
[HCTR][15:05:03][INFO][RK0][main]: Use mixed precision: False
[HCTR][15:05:03][INFO][RK0][main]: Use cuda graph: True
[HCTR][15:05:03][INFO][RK0][main]: Max batchsize: 256
[HCTR][15:05:03][INFO][RK0][main]: Use I64 input key: True
[HCTR][15:05:03][INFO][RK0][main]: start create embedding for inference
[HCTR][15:05:03][INFO][RK0][main]: sparse_input name data1
[HCTR][15:05:03][INFO][RK0][main]: sparse_input name data2
[HCTR][15:05:03][INFO][RK0][main]: create embedding for inference success
[HCTR][15:05:03][INFO][RK0][main]: Inference stage skip MultiCrossEntropyLoss layer, replaced by Sigmoid layer
[HCTR][15:05:03][INFO][RK0][main]: Global seed is 355752151
[HCTR][15:05:03][INFO][RK0][main]: Device to NUMA mapping:
  GPU 3 -&gt;  node 0
[HCTR][15:05:03][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.
[HCTR][15:05:03][INFO][RK0][main]: Start all2all warmup
[HCTR][15:05:03][INFO][RK0][main]: End all2all warmup
[HCTR][15:05:03][INFO][RK0][main]: Create inference session on device: 3
[HCTR][15:05:03][INFO][RK0][main]: Model name: multi_hot
[HCTR][15:05:03][INFO][RK0][main]: Use mixed precision: False
[HCTR][15:05:03][INFO][RK0][main]: Use cuda graph: True
[HCTR][15:05:03][INFO][RK0][main]: Max batchsize: 256
[HCTR][15:05:03][INFO][RK0][main]: Use I64 input key: True
[HCTR][15:05:03][INFO][RK0][main]: start create embedding for inference
[HCTR][15:05:03][INFO][RK0][main]: sparse_input name data1
[HCTR][15:05:03][INFO][RK0][main]: sparse_input name data2
[HCTR][15:05:03][INFO][RK0][main]: create embedding for inference success
[HCTR][15:05:03][INFO][RK0][main]: Inference stage skip MultiCrossEntropyLoss layer, replaced by Sigmoid layer
[HCTR][15:05:03][INFO][RK0][main]: Global seed is 3474526165
[HCTR][15:05:03][INFO][RK0][main]: Device to NUMA mapping:
  GPU 0 -&gt;  node 0
[HCTR][15:05:03][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.
[HCTR][15:05:03][INFO][RK0][main]: Start all2all warmup
[HCTR][15:05:03][INFO][RK0][main]: End all2all warmup
[HCTR][15:05:03][INFO][RK0][main]: Vocabulary size: 30000

pred:  [[0.6733939  0.43605337]
 [0.5189075  0.4978796 ]
 [0.39680484 0.16554658]
 ...
 [0.3779142  0.669542  ]
 [0.46529922 0.44098482]
 [0.58435297 0.45384815]]
grount_truth:  [0.673394 0.436053 0.518908 ... 0.440985 0.584353 0.453848]
mse:  0.0012302037921078574
</pre></div></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hugectr_criteo.html" class="btn btn-neutral float-left" title="HugeCTR Python Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hps_demo.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server Demo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: master
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="multi_gpu_offline_inference.html">master</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>