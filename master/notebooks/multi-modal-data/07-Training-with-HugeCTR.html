<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training HugeCTR Model with Pretrained Embeddings &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="HugeCTR API Documentation" href="../../api/index.html" />
    <link rel="prev" title="ETL with NVTabular" href="06-ETL-with-NVTabular.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HugeCTR Library</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_feature_details_intro.html">Features in Detail</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../hugectr_example_notebooks.html">Example Notebooks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../hugectr_wdl_prediction.html">HugeCTR Wide and Deep Model with Criteo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hugectr2onnx_demo.html">HugeCTR to ONNX Converter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../continuous_training.html">HugeCTR Continuous Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ecommerce-example.html">Merlin ETL, training and inference demo on the e-Commerce behavior data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../movie-lens-example.html">HugeCTR demo on Movie lens data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hugectr_criteo.html">HugeCTR Python Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../multi_gpu_offline_inference.html">Multi-GPU Offline Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hps_demo.html">Hierarchical Parameter Server Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="00-Intro.html">Training Recommender Systems on Multi-modal Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="01-Download-Convert.html">MovieLens-25M: Download and Convert</a></li>
<li class="toctree-l2"><a class="reference internal" href="03-Feature-Extraction-Poster.html">Movie Poster Feature Extraction with ResNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="04-Feature-Extraction-Text.html">Movie Synopsis Feature Extraction with Bart text summarization</a></li>
<li class="toctree-l2"><a class="reference internal" href="04-Feature-Extraction-Text.html#Download-pretrained-BART-model">Download pretrained BART model</a></li>
<li class="toctree-l2"><a class="reference internal" href="05-Create-Feature-Store.html">Creating Multi-Modal Movie Feature Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="06-ETL-with-NVTabular.html">ETL with NVTabular</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Training HugeCTR Model with Pretrained Embeddings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Loading-pretrained-movie-features-into-non-trainable-embedding-layer">Loading pretrained movie features into non-trainable embedding layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Define-and-train-model">Define and train model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../additional_resources.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../hugectr_example_notebooks.html">HugeCTR Example Notebooks</a> &raquo;</li>
      <li>Training HugeCTR Model with Pretrained Embeddings</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Copyright 2021 NVIDIA Corporation. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
</pre></div>
</div>
</div>
<p><img alt="f4a9dd11ea614fbab12419756220011d" src="http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png" /></p>
<div class="section" id="Training-HugeCTR-Model-with-Pretrained-Embeddings">
<h1>Training HugeCTR Model with Pretrained Embeddings<a class="headerlink" href="#Training-HugeCTR-Model-with-Pretrained-Embeddings" title="Permalink to this headline"></a></h1>
<p>In this notebook, we will train a deep neural network for predicting user’s rating (binary target with 1 for ratings <code class="docutils literal notranslate"><span class="pre">&gt;3</span></code> and 0 for ratings <code class="docutils literal notranslate"><span class="pre">&lt;=3</span></code>). The two categorical features are <code class="docutils literal notranslate"><span class="pre">userId</span></code> and <code class="docutils literal notranslate"><span class="pre">movieId</span></code>.</p>
<p>We will also make use of movie’s pretrained embeddings, extracted in the previous notebooks.</p>
<div class="section" id="Loading-pretrained-movie-features-into-non-trainable-embedding-layer">
<h2>Loading pretrained movie features into non-trainable embedding layer<a class="headerlink" href="#Loading-pretrained-movie-features-into-non-trainable-embedding-layer" title="Permalink to this headline"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># loading NVTabular movie encoding
import pandas as pd
import os

INPUT_DATA_DIR = &#39;./data&#39;
movie_mapping = pd.read_parquet(os.path.join(INPUT_DATA_DIR, &quot;workflow-hugectr/categories/unique.movieId.parquet&quot;))
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>movie_mapping.tail()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>movieId</th>
      <th>movieId_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>56581</th>
      <td>209155</td>
      <td>1</td>
    </tr>
    <tr>
      <th>56582</th>
      <td>209157</td>
      <td>1</td>
    </tr>
    <tr>
      <th>56583</th>
      <td>209159</td>
      <td>1</td>
    </tr>
    <tr>
      <th>56584</th>
      <td>209169</td>
      <td>1</td>
    </tr>
    <tr>
      <th>56585</th>
      <td>209171</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>feature_df = pd.read_parquet(&#39;feature_df.parquet&#39;)
print(feature_df.shape)
feature_df.head()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(62423, 3073)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>movieId</th>
      <th>poster_feature_0</th>
      <th>poster_feature_1</th>
      <th>poster_feature_2</th>
      <th>poster_feature_3</th>
      <th>poster_feature_4</th>
      <th>poster_feature_5</th>
      <th>poster_feature_6</th>
      <th>poster_feature_7</th>
      <th>poster_feature_8</th>
      <th>...</th>
      <th>text_feature_1014</th>
      <th>text_feature_1015</th>
      <th>text_feature_1016</th>
      <th>text_feature_1017</th>
      <th>text_feature_1018</th>
      <th>text_feature_1019</th>
      <th>text_feature_1020</th>
      <th>text_feature_1021</th>
      <th>text_feature_1022</th>
      <th>text_feature_1023</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.026260</td>
      <td>0.857608</td>
      <td>0.410247</td>
      <td>0.066654</td>
      <td>0.382803</td>
      <td>0.899998</td>
      <td>0.511562</td>
      <td>0.592291</td>
      <td>0.565434</td>
      <td>...</td>
      <td>0.636716</td>
      <td>0.578369</td>
      <td>0.996169</td>
      <td>0.402107</td>
      <td>0.412318</td>
      <td>0.859952</td>
      <td>0.293852</td>
      <td>0.341114</td>
      <td>0.727113</td>
      <td>0.085829</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.0</td>
      <td>0.141265</td>
      <td>0.721758</td>
      <td>0.679958</td>
      <td>0.955634</td>
      <td>0.391091</td>
      <td>0.324611</td>
      <td>0.505211</td>
      <td>0.258331</td>
      <td>0.048264</td>
      <td>...</td>
      <td>0.161505</td>
      <td>0.431864</td>
      <td>0.836532</td>
      <td>0.525013</td>
      <td>0.654566</td>
      <td>0.823841</td>
      <td>0.818313</td>
      <td>0.856280</td>
      <td>0.638048</td>
      <td>0.685537</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.0</td>
      <td>0.119418</td>
      <td>0.911146</td>
      <td>0.470762</td>
      <td>0.762258</td>
      <td>0.626335</td>
      <td>0.768947</td>
      <td>0.241833</td>
      <td>0.775992</td>
      <td>0.236340</td>
      <td>...</td>
      <td>0.865548</td>
      <td>0.387806</td>
      <td>0.668321</td>
      <td>0.552122</td>
      <td>0.750238</td>
      <td>0.863707</td>
      <td>0.382173</td>
      <td>0.894487</td>
      <td>0.565142</td>
      <td>0.164083</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.0</td>
      <td>0.538184</td>
      <td>0.980678</td>
      <td>0.643513</td>
      <td>0.928519</td>
      <td>0.794906</td>
      <td>0.201022</td>
      <td>0.744666</td>
      <td>0.962188</td>
      <td>0.915320</td>
      <td>...</td>
      <td>0.777534</td>
      <td>0.904200</td>
      <td>0.167337</td>
      <td>0.875194</td>
      <td>0.180481</td>
      <td>0.815904</td>
      <td>0.808288</td>
      <td>0.036711</td>
      <td>0.902779</td>
      <td>0.580946</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>0.772951</td>
      <td>0.239788</td>
      <td>0.061874</td>
      <td>0.162997</td>
      <td>0.388310</td>
      <td>0.236311</td>
      <td>0.162757</td>
      <td>0.207134</td>
      <td>0.111078</td>
      <td>...</td>
      <td>0.250022</td>
      <td>0.335043</td>
      <td>0.091674</td>
      <td>0.121507</td>
      <td>0.418124</td>
      <td>0.150020</td>
      <td>0.803506</td>
      <td>0.059504</td>
      <td>0.002342</td>
      <td>0.932321</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 3073 columns</p>
</div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>feature_df.set_index(&#39;movieId&#39;, inplace=True)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from tqdm import tqdm
import numpy as np

num_tokens = len(movie_mapping)
embedding_dim = 2048+1024
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))

print(&quot;Loading pretrained embedding matrix...&quot;)
for i, row in tqdm(movie_mapping.iterrows(), total=len(movie_mapping)):
    movieId = row[&#39;movieId&#39;]
    if movieId in feature_df.index:
        embedding_vector = feature_df.loc[movieId]
        # embedding found
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1
print(&quot;Found features for %d movies (%d misses)&quot; % (hits, misses))
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loading pretrained embedding matrix...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|████████████████████████████████████| 56586/56586 [00:14&lt;00:00, 3967.46it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found features for 56585 movies (1 misses)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>embedding_dim
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
3072
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>embedding_matrix
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.17294852, 0.15285189, 0.26095702, ..., 0.75369112, 0.29602144,
        0.78917433],
       [0.13539355, 0.84843078, 0.70951219, ..., 0.10441725, 0.72871966,
        0.11719463],
       ...,
       [0.18514273, 0.72422918, 0.04273015, ..., 0.1404219 , 0.54169348,
        0.96875489],
       [0.08307642, 0.3673532 , 0.15777258, ..., 0.01297393, 0.36267638,
        0.14848055],
       [0.82188376, 0.56516905, 0.70838085, ..., 0.45119769, 0.9273439 ,
        0.42464321]])
</pre></div></div>
</div>
<p>Next, we write the pretrained embedding to a raw format supported by HugeCTR.</p>
<p>Note: As of version 3.2, HugeCTR only supports a maximum embedding size of 1024. Hence, we shall be using the first 512 elememt of image embedding plus 512 element of text embedding.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import struct

PRETRAINED_EMBEDDING_SIZE = 1024

def convert_pretrained_embeddings_to_sparse_model(keys, pre_trained_sparse_embeddings, hugectr_sparse_model, embedding_vec_size):
    os.system(&quot;mkdir -p {}&quot;.format(hugectr_sparse_model))
    with open(&quot;{}/key&quot;.format(hugectr_sparse_model), &#39;wb&#39;) as key_file, \
        open(&quot;{}/emb_vector&quot;.format(hugectr_sparse_model), &#39;wb&#39;) as vec_file:

        for i, key in enumerate(keys):
            vec = np.concatenate([pre_trained_sparse_embeddings[i,:int(PRETRAINED_EMBEDDING_SIZE/2)], pre_trained_sparse_embeddings[i, 1024:1024+int(PRETRAINED_EMBEDDING_SIZE/2)]])
            key_struct = struct.pack(&#39;q&#39;, key)
            vec_struct = struct.pack(str(embedding_vec_size) + &quot;f&quot;, *vec)
            key_file.write(key_struct)
            vec_file.write(vec_struct)

keys = list(movie_mapping.index)
convert_pretrained_embeddings_to_sparse_model(keys, embedding_matrix, &#39;hugectr_pretrained_embedding.model&#39;, embedding_vec_size=PRETRAINED_EMBEDDING_SIZE) # HugeCTR not supporting embedding size &gt; 1024
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-and-train-model">
<h2>Define and train model<a class="headerlink" href="#Define-and-train-model" title="Permalink to this headline"></a></h2>
<p>In this section, we define and train the model. The model comprise trainable embedding layers for categorical features (<code class="docutils literal notranslate"><span class="pre">userId</span></code>, <code class="docutils literal notranslate"><span class="pre">movieId</span></code>) and pretrained (non-trainable) embedding layer for movie features.</p>
<p>We will write the model to <code class="docutils literal notranslate"><span class="pre">./model.py</span></code> and execute it afterwards.</p>
<p>First, we need the cardinalities of each categorical feature to assign as <code class="docutils literal notranslate"><span class="pre">slot_size_array</span></code> in the model below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import nvtabular as nvt
from nvtabular.ops import get_embedding_sizes

workflow = nvt.Workflow.load(os.path.join(INPUT_DATA_DIR, &quot;workflow-hugectr&quot;))

embeddings = get_embedding_sizes(workflow)
print(embeddings)

#{&#39;userId&#39;: (162542, 512), &#39;movieId&#39;: (56586, 512), &#39;movieId_duplicate&#39;: (56586, 512)}
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;userId&#39;: (162542, 512), &#39;movieId&#39;: (56586, 512), &#39;movieId_duplicate&#39;: (56586, 512)}
</pre></div></div>
</div>
<p>We use <code class="docutils literal notranslate"><span class="pre">graph_to_json</span></code> to convert the model to a JSON configuration, required for the inference.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%writefile &#39;./model.py&#39;

import hugectr
from mpi4py import MPI  # noqa
INPUT_DATA_DIR = &#39;./data/&#39;

solver = hugectr.CreateSolver(
    vvgpu=[[0]],
    batchsize=2048,
    batchsize_eval=2048,
    max_eval_batches=160,
    i64_input_key=True,
    use_mixed_precision=False,
    repeat_dataset=True,
)
optimizer = hugectr.CreateOptimizer(optimizer_type=hugectr.Optimizer_t.Adam)
reader = hugectr.DataReaderParams(
    data_reader_type=hugectr.DataReaderType_t.Parquet,
    source=[INPUT_DATA_DIR + &quot;train-hugectr/_file_list.txt&quot;],
    eval_source=INPUT_DATA_DIR + &quot;valid-hugectr/_file_list.txt&quot;,
    check_type=hugectr.Check_t.Non,
    slot_size_array=[162542, 56586, 21, 56586],
)

model = hugectr.Model(solver, reader, optimizer)

model.add(
    hugectr.Input(
        label_dim=1,
        label_name=&quot;label&quot;,
        dense_dim=0,
        dense_name=&quot;dense&quot;,
        data_reader_sparse_param_array=[
            hugectr.DataReaderSparseParam(&quot;data1&quot;, nnz_per_slot=[1, 1, 2], is_fixed_length=False, slot_num=3),
            hugectr.DataReaderSparseParam(&quot;movieId&quot;, nnz_per_slot=[1], is_fixed_length=True, slot_num=1)
        ],
    )
)
model.add(
    hugectr.SparseEmbedding(
        embedding_type=hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,
        workspace_size_per_gpu_in_mb=3000,
        embedding_vec_size=16,
        combiner=&quot;sum&quot;,
        sparse_embedding_name=&quot;sparse_embedding1&quot;,
        bottom_name=&quot;data1&quot;,
        optimizer=optimizer,
    )
)

# pretrained embedding
model.add(
    hugectr.SparseEmbedding(
        embedding_type=hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash,
        workspace_size_per_gpu_in_mb=3000,
        embedding_vec_size=1024,
        combiner=&quot;sum&quot;,
        sparse_embedding_name=&quot;pretrained_embedding&quot;,
        bottom_name=&quot;movieId&quot;,
        optimizer=optimizer,
    )
)

model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,
                            bottom_names = [&quot;sparse_embedding1&quot;],
                            top_names = [&quot;reshape1&quot;],
                            leading_dim=48))

model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,
                            bottom_names = [&quot;pretrained_embedding&quot;],
                            top_names = [&quot;reshape2&quot;],
                            leading_dim=1024))

model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,
                            bottom_names = [&quot;reshape1&quot;, &quot;reshape2&quot;],
                            top_names = [&quot;concat1&quot;]))

model.add(
    hugectr.DenseLayer(
        layer_type=hugectr.Layer_t.InnerProduct,
        bottom_names=[&quot;concat1&quot;],
        top_names=[&quot;fc1&quot;],
        num_output=128,
    )
)
model.add(
    hugectr.DenseLayer(
        layer_type=hugectr.Layer_t.ReLU,
        bottom_names=[&quot;fc1&quot;],
        top_names=[&quot;relu1&quot;],
    )
)
model.add(
    hugectr.DenseLayer(
        layer_type=hugectr.Layer_t.InnerProduct,
        bottom_names=[&quot;relu1&quot;],
        top_names=[&quot;fc2&quot;],
        num_output=128,
    )
)
model.add(
    hugectr.DenseLayer(
        layer_type=hugectr.Layer_t.ReLU,
        bottom_names=[&quot;fc2&quot;],
        top_names=[&quot;relu2&quot;],
    )
)
model.add(
    hugectr.DenseLayer(
        layer_type=hugectr.Layer_t.InnerProduct,
        bottom_names=[&quot;relu2&quot;],
        top_names=[&quot;fc3&quot;],
        num_output=1,
    )
)
model.add(
    hugectr.DenseLayer(
        layer_type=hugectr.Layer_t.BinaryCrossEntropyLoss,
        bottom_names=[&quot;fc3&quot;, &quot;label&quot;],
        top_names=[&quot;loss&quot;],
    )
)
model.compile()
model.summary()

# Load the pretrained embedding layer
model.load_sparse_weights({&quot;pretrained_embedding&quot;: &quot;./hugectr_pretrained_embedding.model&quot;})
model.freeze_embedding(&quot;pretrained_embedding&quot;)

model.fit(max_iter=10001, display=100, eval_interval=200, snapshot=5000)
model.graph_to_json(graph_config_file=&quot;hugectr-movielens.json&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Overwriting ./model.py
</pre></div></div>
</div>
<p>We train our model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!python model.py
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
HugeCTR Version: 3.2
====================================================Model Init=====================================================
[HUGECTR][01:09:00][INFO][RANK0]: Global seed is 476440390
[HUGECTR][01:09:00][INFO][RANK0]: Device to NUMA mapping:
  GPU 0 -&gt;  node 0

[HUGECTR][01:09:01][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.
[HUGECTR][01:09:01][INFO][RANK0]: Start all2all warmup
[HUGECTR][01:09:01][INFO][RANK0]: End all2all warmup
[HUGECTR][01:09:01][INFO][RANK0]: Using All-reduce algorithm: NCCL
[HUGECTR][01:09:01][INFO][RANK0]: Device 0: Tesla V100-SXM2-16GB
[HUGECTR][01:09:01][INFO][RANK0]: num of DataReader workers: 1
[HUGECTR][01:09:01][INFO][RANK0]: Vocabulary size: 275735
[HUGECTR][01:09:01][INFO][RANK0]: max_vocabulary_size_per_gpu_=16384000
[HUGECTR][01:09:01][INFO][RANK0]: max_vocabulary_size_per_gpu_=256000
[HUGECTR][01:09:01][INFO][RANK0]: Graph analysis to resolve tensor dependency
===================================================Model Compile===================================================
[HUGECTR][01:09:04][INFO][RANK0]: gpu0 start to init embedding
[HUGECTR][01:09:04][INFO][RANK0]: gpu0 init embedding done
[HUGECTR][01:09:04][INFO][RANK0]: gpu0 start to init embedding
[HUGECTR][01:09:04][INFO][RANK0]: gpu0 init embedding done
[HUGECTR][01:09:04][INFO][RANK0]: Starting AUC NCCL warm-up
[HUGECTR][01:09:04][INFO][RANK0]: Warm-up done
===================================================Model Summary===================================================
label                                   Dense                         Sparse
label                                   dense                          data1,movieId
(None, 1)                               (None, 0)
——————————————————————————————————————————————————————————————————————————————————————————————————————————————————
Layer Type                              Input Name                    Output Name                   Output Shape
——————————————————————————————————————————————————————————————————————————————————————————————————————————————————
LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 3, 16)
------------------------------------------------------------------------------------------------------------------
DistributedSlotSparseEmbeddingHash      movieId                       pretrained_embedding          (None, 1, 1024)
------------------------------------------------------------------------------------------------------------------
Reshape                                 sparse_embedding1             reshape1                      (None, 48)
------------------------------------------------------------------------------------------------------------------
Reshape                                 pretrained_embedding          reshape2                      (None, 1024)
------------------------------------------------------------------------------------------------------------------
Concat                                  reshape1                      concat1                       (None, 1072)
                                        reshape2
------------------------------------------------------------------------------------------------------------------
InnerProduct                            concat1                       fc1                           (None, 128)
------------------------------------------------------------------------------------------------------------------
ReLU                                    fc1                           relu1                         (None, 128)
------------------------------------------------------------------------------------------------------------------
InnerProduct                            relu1                         fc2                           (None, 128)
------------------------------------------------------------------------------------------------------------------
ReLU                                    fc2                           relu2                         (None, 128)
------------------------------------------------------------------------------------------------------------------
InnerProduct                            relu2                         fc3                           (None, 1)
------------------------------------------------------------------------------------------------------------------
BinaryCrossEntropyLoss                  fc3                           loss
                                        label
------------------------------------------------------------------------------------------------------------------
[HUGECTR][01:09:04][INFO][RANK0]: Loading sparse model: ./hugectr_pretrained_embedding.model
=====================================================Model Fit=====================================================
[HUGECTR][01:09:06][INFO][RANK0]: Use non-epoch mode with number of iterations: 10001
[HUGECTR][01:09:06][INFO][RANK0]: Training batchsize: 2048, evaluation batchsize: 2048
[HUGECTR][01:09:06][INFO][RANK0]: Evaluation interval: 200, snapshot interval: 5000
[HUGECTR][01:09:06][INFO][RANK0]: Dense network trainable: True
[HUGECTR][01:09:06][INFO][RANK0]: Sparse embedding pretrained_embedding trainable: False
[HUGECTR][01:09:06][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True
[HUGECTR][01:09:06][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True
[HUGECTR][01:09:06][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000
[HUGECTR][01:09:06][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000
[HUGECTR][01:09:06][INFO][RANK0]: Training source file: ./data/train-hugectr/_file_list.txt
[HUGECTR][01:09:06][INFO][RANK0]: Evaluation source file: ./data/valid-hugectr/_file_list.txt
[HUGECTR][01:09:08][INFO][RANK0]: Iter: 100 Time(100 iters): 2.297110s Loss: 0.581705 lr:0.001000
[HUGECTR][01:09:11][INFO][RANK0]: Iter: 200 Time(100 iters): 2.274680s Loss: 0.574425 lr:0.001000
[HUGECTR][01:09:11][INFO][RANK0]: Evaluation, AUC: 0.746443
[HUGECTR][01:09:11][INFO][RANK0]: Eval Time for 160 iters: 0.054157s
[HUGECTR][01:09:13][INFO][RANK0]: Iter: 300 Time(100 iters): 2.332273s Loss: 0.564224 lr:0.001000
[HUGECTR][01:09:15][INFO][RANK0]: Iter: 400 Time(100 iters): 2.277900s Loss: 0.550730 lr:0.001000
[HUGECTR][01:09:15][INFO][RANK0]: Evaluation, AUC: 0.764630
[HUGECTR][01:09:15][INFO][RANK0]: Eval Time for 160 iters: 0.054009s
[HUGECTR][01:09:18][INFO][RANK0]: Iter: 500 Time(100 iters): 2.434429s Loss: 0.536507 lr:0.001000
[HUGECTR][01:09:20][INFO][RANK0]: Iter: 600 Time(100 iters): 2.279014s Loss: 0.525059 lr:0.001000
[HUGECTR][01:09:20][INFO][RANK0]: Evaluation, AUC: 0.773702
[HUGECTR][01:09:20][INFO][RANK0]: Eval Time for 160 iters: 0.054287s
[HUGECTR][01:09:22][INFO][RANK0]: Iter: 700 Time(100 iters): 2.335757s Loss: 0.532503 lr:0.001000
[HUGECTR][01:09:25][INFO][RANK0]: Iter: 800 Time(100 iters): 2.278661s Loss: 0.526352 lr:0.001000
[HUGECTR][01:09:25][INFO][RANK0]: Evaluation, AUC: 0.779897
[HUGECTR][01:09:25][INFO][RANK0]: Eval Time for 160 iters: 0.167787s
[HUGECTR][01:09:27][INFO][RANK0]: Iter: 900 Time(100 iters): 2.447136s Loss: 0.547141 lr:0.001000
[HUGECTR][01:09:29][INFO][RANK0]: Iter: 1000 Time(100 iters): 2.376035s Loss: 0.548916 lr:0.001000
[HUGECTR][01:09:30][INFO][RANK0]: Evaluation, AUC: 0.784775
[HUGECTR][01:09:30][INFO][RANK0]: Eval Time for 160 iters: 0.054224s
[HUGECTR][01:09:32][INFO][RANK0]: Iter: 1100 Time(100 iters): 2.334735s Loss: 0.540766 lr:0.001000
[HUGECTR][01:09:34][INFO][RANK0]: Iter: 1200 Time(100 iters): 2.277728s Loss: 0.515882 lr:0.001000
[HUGECTR][01:09:34][INFO][RANK0]: Evaluation, AUC: 0.786808
[HUGECTR][01:09:34][INFO][RANK0]: Eval Time for 160 iters: 0.054551s
[HUGECTR][01:09:36][INFO][RANK0]: Iter: 1300 Time(100 iters): 2.336372s Loss: 0.531510 lr:0.001000
[HUGECTR][01:09:39][INFO][RANK0]: Iter: 1400 Time(100 iters): 2.277408s Loss: 0.511901 lr:0.001000
[HUGECTR][01:09:39][INFO][RANK0]: Evaluation, AUC: 0.791416
[HUGECTR][01:09:39][INFO][RANK0]: Eval Time for 160 iters: 0.165986s
[HUGECTR][01:09:41][INFO][RANK0]: Iter: 1500 Time(100 iters): 2.554217s Loss: 0.522047 lr:0.001000
[HUGECTR][01:09:44][INFO][RANK0]: Iter: 1600 Time(100 iters): 2.279548s Loss: 0.540521 lr:0.001000
[HUGECTR][01:09:44][INFO][RANK0]: Evaluation, AUC: 0.793460
[HUGECTR][01:09:44][INFO][RANK0]: Eval Time for 160 iters: 0.054801s
[HUGECTR][01:09:46][INFO][RANK0]: Iter: 1700 Time(100 iters): 2.336303s Loss: 0.525447 lr:0.001000
[HUGECTR][01:09:48][INFO][RANK0]: Iter: 1800 Time(100 iters): 2.278906s Loss: 0.523558 lr:0.001000
[HUGECTR][01:09:48][INFO][RANK0]: Evaluation, AUC: 0.793137
[HUGECTR][01:09:48][INFO][RANK0]: Eval Time for 160 iters: 0.054431s
[HUGECTR][01:09:51][INFO][RANK0]: Iter: 1900 Time(100 iters): 2.336023s Loss: 0.511348 lr:0.001000
[HUGECTR][01:09:53][INFO][RANK0]: Iter: 2000 Time(100 iters): 2.384979s Loss: 0.515268 lr:0.001000
[HUGECTR][01:09:53][INFO][RANK0]: Evaluation, AUC: 0.796599
[HUGECTR][01:09:53][INFO][RANK0]: Eval Time for 160 iters: 0.172160s
[HUGECTR][01:09:55][INFO][RANK0]: Iter: 2100 Time(100 iters): 2.453174s Loss: 0.526615 lr:0.001000
[HUGECTR][01:09:58][INFO][RANK0]: Iter: 2200 Time(100 iters): 2.278781s Loss: 0.536789 lr:0.001000
[HUGECTR][01:09:58][INFO][RANK0]: Evaluation, AUC: 0.798459
[HUGECTR][01:09:58][INFO][RANK0]: Eval Time for 160 iters: 0.054509s
[HUGECTR][01:10:00][INFO][RANK0]: Iter: 2300 Time(100 iters): 2.335596s Loss: 0.508902 lr:0.001000
[HUGECTR][01:10:02][INFO][RANK0]: Iter: 2400 Time(100 iters): 2.277901s Loss: 0.520411 lr:0.001000
[HUGECTR][01:10:02][INFO][RANK0]: Evaluation, AUC: 0.798726
[HUGECTR][01:10:02][INFO][RANK0]: Eval Time for 160 iters: 0.054518s
[HUGECTR][01:10:05][INFO][RANK0]: Iter: 2500 Time(100 iters): 2.444557s Loss: 0.490832 lr:0.001000
[HUGECTR][01:10:07][INFO][RANK0]: Iter: 2600 Time(100 iters): 2.279310s Loss: 0.507799 lr:0.001000
[HUGECTR][01:10:07][INFO][RANK0]: Evaluation, AUC: 0.801325
[HUGECTR][01:10:07][INFO][RANK0]: Eval Time for 160 iters: 0.164203s
[HUGECTR][01:10:10][INFO][RANK0]: Iter: 2700 Time(100 iters): 2.443310s Loss: 0.519460 lr:0.001000
[HUGECTR][01:10:12][INFO][RANK0]: Iter: 2800 Time(100 iters): 2.277569s Loss: 0.512426 lr:0.001000
[HUGECTR][01:10:12][INFO][RANK0]: Evaluation, AUC: 0.800731
[HUGECTR][01:10:12][INFO][RANK0]: Eval Time for 160 iters: 0.054590s
[HUGECTR][01:10:14][INFO][RANK0]: Iter: 2900 Time(100 iters): 2.336213s Loss: 0.512216 lr:0.001000
[HUGECTR][01:10:17][INFO][RANK0]: Iter: 3000 Time(100 iters): 2.384833s Loss: 0.522102 lr:0.001000
[HUGECTR][01:10:17][INFO][RANK0]: Evaluation, AUC: 0.803801
[HUGECTR][01:10:17][INFO][RANK0]: Eval Time for 160 iters: 0.054133s
[HUGECTR][01:10:19][INFO][RANK0]: Iter: 3100 Time(100 iters): 2.334245s Loss: 0.507463 lr:0.001000
[HUGECTR][01:10:21][INFO][RANK0]: Iter: 3200 Time(100 iters): 2.279046s Loss: 0.526148 lr:0.001000
[HUGECTR][01:10:21][INFO][RANK0]: Evaluation, AUC: 0.802950
[HUGECTR][01:10:21][INFO][RANK0]: Eval Time for 160 iters: 0.070003s
[HUGECTR][01:10:24][INFO][RANK0]: Iter: 3300 Time(100 iters): 2.352114s Loss: 0.504611 lr:0.001000
[HUGECTR][01:10:26][INFO][RANK0]: Iter: 3400 Time(100 iters): 2.277292s Loss: 0.502907 lr:0.001000
[HUGECTR][01:10:26][INFO][RANK0]: Evaluation, AUC: 0.804364
[HUGECTR][01:10:26][INFO][RANK0]: Eval Time for 160 iters: 0.054315s
[HUGECTR][01:10:28][INFO][RANK0]: Iter: 3500 Time(100 iters): 2.442956s Loss: 0.512927 lr:0.001000
[HUGECTR][01:10:31][INFO][RANK0]: Iter: 3600 Time(100 iters): 2.277974s Loss: 0.519042 lr:0.001000
[HUGECTR][01:10:31][INFO][RANK0]: Evaluation, AUC: 0.806404
[HUGECTR][01:10:31][INFO][RANK0]: Eval Time for 160 iters: 0.054291s
[HUGECTR][01:10:33][INFO][RANK0]: Iter: 3700 Time(100 iters): 2.335365s Loss: 0.499368 lr:0.001000
[HUGECTR][01:10:35][INFO][RANK0]: Iter: 3800 Time(100 iters): 2.277786s Loss: 0.509683 lr:0.001000
[HUGECTR][01:10:35][INFO][RANK0]: Evaluation, AUC: 0.805164
[HUGECTR][01:10:35][INFO][RANK0]: Eval Time for 160 iters: 0.064908s
[HUGECTR][01:10:38][INFO][RANK0]: Iter: 3900 Time(100 iters): 2.344106s Loss: 0.508182 lr:0.001000
[HUGECTR][01:10:40][INFO][RANK0]: Iter: 4000 Time(100 iters): 2.387872s Loss: 0.493841 lr:0.001000
[HUGECTR][01:10:40][INFO][RANK0]: Evaluation, AUC: 0.808367
[HUGECTR][01:10:40][INFO][RANK0]: Eval Time for 160 iters: 0.054222s
[HUGECTR][01:10:42][INFO][RANK0]: Iter: 4100 Time(100 iters): 2.335361s Loss: 0.508106 lr:0.001000
[HUGECTR][01:10:45][INFO][RANK0]: Iter: 4200 Time(100 iters): 2.278802s Loss: 0.519000 lr:0.001000
[HUGECTR][01:10:45][INFO][RANK0]: Evaluation, AUC: 0.808897
[HUGECTR][01:10:45][INFO][RANK0]: Eval Time for 160 iters: 0.054320s
[HUGECTR][01:10:47][INFO][RANK0]: Iter: 4300 Time(100 iters): 2.334094s Loss: 0.502797 lr:0.001000
[HUGECTR][01:10:49][INFO][RANK0]: Iter: 4400 Time(100 iters): 2.388990s Loss: 0.508890 lr:0.001000
[HUGECTR][01:10:49][INFO][RANK0]: Evaluation, AUC: 0.809649
[HUGECTR][01:10:49][INFO][RANK0]: Eval Time for 160 iters: 0.074584s
[HUGECTR][01:10:52][INFO][RANK0]: Iter: 4500 Time(100 iters): 2.355005s Loss: 0.505778 lr:0.001000
[HUGECTR][01:10:54][INFO][RANK0]: Iter: 4600 Time(100 iters): 2.277275s Loss: 0.532776 lr:0.001000
[HUGECTR][01:10:54][INFO][RANK0]: Evaluation, AUC: 0.810962
[HUGECTR][01:10:54][INFO][RANK0]: Eval Time for 160 iters: 0.054498s
[HUGECTR][01:10:56][INFO][RANK0]: Iter: 4700 Time(100 iters): 2.335553s Loss: 0.503001 lr:0.001000
[HUGECTR][01:10:59][INFO][RANK0]: Iter: 4800 Time(100 iters): 2.279237s Loss: 0.495762 lr:0.001000
[HUGECTR][01:10:59][INFO][RANK0]: Evaluation, AUC: 0.808618
[HUGECTR][01:10:59][INFO][RANK0]: Eval Time for 160 iters: 0.054287s
[HUGECTR][01:11:01][INFO][RANK0]: Iter: 4900 Time(100 iters): 2.449926s Loss: 0.503213 lr:0.001000
[HUGECTR][01:11:03][INFO][RANK0]: Iter: 5000 Time(100 iters): 2.277141s Loss: 0.481138 lr:0.001000
[HUGECTR][01:11:03][INFO][RANK0]: Evaluation, AUC: 0.810767
[HUGECTR][01:11:03][INFO][RANK0]: Eval Time for 160 iters: 0.064807s
[HUGECTR][01:11:04][INFO][RANK0]: Rank0: Dump hash table from GPU0
[HUGECTR][01:11:04][INFO][RANK0]: Rank0: Write hash table &lt;key,value&gt; pairs to file
[HUGECTR][01:11:04][INFO][RANK0]: Done
[HUGECTR][01:11:04][INFO][RANK0]: Rank0: Write hash table to file
[HUGECTR][01:11:13][INFO][RANK0]: Dumping sparse weights to files, successful
[HUGECTR][01:11:13][INFO][RANK0]: Rank0: Write optimzer state to file
[HUGECTR][01:11:14][INFO][RANK0]: Done
[HUGECTR][01:11:14][INFO][RANK0]: Rank0: Write optimzer state to file
[HUGECTR][01:11:15][INFO][RANK0]: Done
[HUGECTR][01:11:34][INFO][RANK0]: Rank0: Write optimzer state to file
[HUGECTR][01:11:35][INFO][RANK0]: Done
[HUGECTR][01:11:35][INFO][RANK0]: Rank0: Write optimzer state to file
[HUGECTR][01:11:36][INFO][RANK0]: Done
[HUGECTR][01:11:55][INFO][RANK0]: Dumping sparse optimzer states to files, successful
[HUGECTR][01:11:55][INFO][RANK0]: Dumping dense weights to file, successful
[HUGECTR][01:11:55][INFO][RANK0]: Dumping dense optimizer states to file, successful
[HUGECTR][01:11:55][INFO][RANK0]: Dumping untrainable weights to file, successful
[HUGECTR][01:11:57][INFO][RANK0]: Iter: 5100 Time(100 iters): 53.630313s Loss: 0.485568 lr:0.001000
[HUGECTR][01:11:59][INFO][RANK0]: Iter: 5200 Time(100 iters): 2.278359s Loss: 0.518924 lr:0.001000
[HUGECTR][01:11:59][INFO][RANK0]: Evaluation, AUC: 0.811217
[HUGECTR][01:11:59][INFO][RANK0]: Eval Time for 160 iters: 0.054624s
[HUGECTR][01:12:02][INFO][RANK0]: Iter: 5300 Time(100 iters): 2.336246s Loss: 0.516505 lr:0.001000
[HUGECTR][01:12:04][INFO][RANK0]: Iter: 5400 Time(100 iters): 2.384571s Loss: 0.512404 lr:0.001000
[HUGECTR][01:12:04][INFO][RANK0]: Evaluation, AUC: 0.811464
[HUGECTR][01:12:04][INFO][RANK0]: Eval Time for 160 iters: 0.054350s
[HUGECTR][01:12:06][INFO][RANK0]: Iter: 5500 Time(100 iters): 2.334675s Loss: 0.500305 lr:0.001000
[HUGECTR][01:12:09][INFO][RANK0]: Iter: 5600 Time(100 iters): 2.279563s Loss: 0.484969 lr:0.001000
</pre></div></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="06-ETL-with-NVTabular.html" class="btn btn-neutral float-left" title="ETL with NVTabular" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../api/index.html" class="btn btn-neutral float-right" title="HugeCTR API Documentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: master
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="07-Training-with-HugeCTR.html">master</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>